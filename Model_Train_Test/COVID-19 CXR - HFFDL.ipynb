{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Flatten, Dropout, Activation, Input, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "print(gpus, cpus)\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_shape = (150, 150, 3)\n",
    "\n",
    "data_x = np.load('../Data/COVID CXR/covid_lung_Unet_Crop_finish_150_X.npy')\n",
    "data_y = np.load('../Data/COVID CXR/covid_lung_Unet_Crop_finish_150_Y.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [k for k in range(data_x.shape[0])]\n",
    "np.random.shuffle(index) \n",
    "\n",
    "\n",
    "data_x2 = data_x[index]\n",
    "data_y2 = data_y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x2 = data_x2 / 255.0\n",
    "data_y2 = tf.keras.utils.to_categorical(data_y2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    _, height, width, channels = x.shape\n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size\n",
    "    x = tf.reshape(\n",
    "        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n",
    "    )\n",
    "    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n",
    "    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, height, width, channels):\n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size\n",
    "    x = tf.reshape(\n",
    "        windows,\n",
    "        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n",
    "    )\n",
    "    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n",
    "    x = tf.reshape(x, shape=(-1, height, width, channels))\n",
    "    return x\n",
    "\n",
    "\n",
    "class DropPath(layers.Layer):\n",
    "    def __init__(self, drop_prob=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x):\n",
    "        input_shape = tf.shape(x)\n",
    "        batch_size = input_shape[0]\n",
    "        rank = x.shape.rank\n",
    "        shape = (batch_size,) + (1,) * (rank - 1)\n",
    "        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n",
    "        path_mask = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, prefix='', **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.prefix = prefix\n",
    "        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias, name=f'{self.prefix}/attn/qkv')\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.proj = layers.Dense(dim, name=f'{self.prefix}/attn/proj')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        num_window_elements = (2 * self.window_size[0] - 1) * (\n",
    "            2 * self.window_size[1] - 1\n",
    "        )\n",
    "        self.relative_position_bias_table = self.add_weight(f'{self.prefix}/attn/relative_position_bias_table',\n",
    "            shape=(num_window_elements, self.num_heads),\n",
    "            initializer=tf.initializers.Zeros(),\n",
    "            trainable=True,\n",
    "        )\n",
    "        coords_h = np.arange(self.window_size[0])\n",
    "        coords_w = np.arange(self.window_size[1])\n",
    "        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n",
    "        coords = np.stack(coords_matrix)\n",
    "        coords_flatten = coords.reshape(2, -1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "        self.relative_position_index = tf.Variable(\n",
    "            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False,\n",
    "            name=f'{self.prefix}/attn/relative_position_index'\n",
    "        )\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        _, size, channels = x.shape\n",
    "        head_dim = channels // self.num_heads\n",
    "        x_qkv = self.qkv(x)\n",
    "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n",
    "        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n",
    "        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n",
    "        q = q * self.scale\n",
    "        k = tf.transpose(k, perm=(0, 1, 3, 2))\n",
    "        attn = q @ k\n",
    "\n",
    "        num_window_elements = self.window_size[0] * self.window_size[1]\n",
    "        relative_position_index_flat = tf.reshape(\n",
    "            self.relative_position_index, shape=(-1,)\n",
    "        )\n",
    "        relative_position_bias = tf.gather(\n",
    "            self.relative_position_bias_table, relative_position_index_flat\n",
    "        )\n",
    "        relative_position_bias = tf.reshape(\n",
    "            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n",
    "        )\n",
    "        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n",
    "        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.get_shape()[0]\n",
    "            mask_float = tf.cast(\n",
    "                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n",
    "            )\n",
    "            attn = (\n",
    "                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n",
    "                + mask_float\n",
    "            )\n",
    "            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n",
    "            attn = tf.keras.activations.softmax(attn, axis=-1)\n",
    "        else:\n",
    "            attn = tf.keras.activations.softmax(attn, axis=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x_qkv = attn @ v\n",
    "        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n",
    "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n",
    "        x_qkv = self.proj(x_qkv)\n",
    "        x_qkv = self.dropout(x_qkv)\n",
    "        return x_qkv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer_Block(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_patch,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        num_mlp=1024,\n",
    "        qkv_bias=True,\n",
    "        dropout_rate=0.0,\n",
    "        prefix='',\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dim = dim  # number of input dimensions\n",
    "        self.num_patch = num_patch  # number of embedded patches\n",
    "        self.num_heads = num_heads  # number of attention heads\n",
    "        self.window_size = window_size  # size of window\n",
    "        self.shift_size = shift_size  # size of window shift\n",
    "        self.num_mlp = num_mlp  # number of MLP nodes\n",
    "        self.prefix = prefix\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-5, name=f'{self.prefix}/norm1')\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=(self.window_size, self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            dropout_rate=dropout_rate,\n",
    "            prefix=self.prefix\n",
    "        )\n",
    "        self.drop_path = DropPath(dropout_rate)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-5, name=f'{self.prefix}/norm2')\n",
    "        \n",
    "        self.mlp = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(num_mlp, name=f'{prefix}/mlp/fc1'),\n",
    "                layers.Activation(tf.keras.activations.gelu),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Dense(dim, name=f'{prefix}/mlp/fc2'),\n",
    "                layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if min(self.num_patch) < self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.num_patch)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.shift_size == 0:\n",
    "            self.attn_mask = None\n",
    "        else:\n",
    "            height, width = self.num_patch\n",
    "            h_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            w_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            mask_array = np.zeros((1, height, width, 1))\n",
    "            count = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    mask_array[:, h, w, :] = count\n",
    "                    count += 1\n",
    "            mask_array = tf.convert_to_tensor(mask_array)\n",
    "\n",
    "            # mask array to windows\n",
    "            mask_windows = window_partition(mask_array, self.window_size)\n",
    "            mask_windows = tf.reshape(\n",
    "                mask_windows, shape=[-1, self.window_size * self.window_size]\n",
    "            )\n",
    "            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n",
    "                mask_windows, axis=2\n",
    "            )\n",
    "            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n",
    "            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n",
    "            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False, name=f'{self.prefix}/attn_mask')\n",
    "\n",
    "    def call(self, x):\n",
    "        height, width = self.num_patch\n",
    "        _, num_patches_before, channels = x.shape\n",
    "        x_skip = x\n",
    "        x = self.norm1(x)\n",
    "        x = tf.reshape(x, shape=(-1, height, width, channels))\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = tf.roll(\n",
    "                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n",
    "            )\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = tf.reshape(\n",
    "            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n",
    "        )\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
    "\n",
    "        attn_windows = tf.reshape(\n",
    "            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n",
    "        )\n",
    "        shifted_x = window_reverse(\n",
    "            attn_windows, self.window_size, height, width, channels\n",
    "        )\n",
    "        if self.shift_size > 0:\n",
    "            x = tf.roll(\n",
    "                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n",
    "            )\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = tf.reshape(x, shape=(-1, height * width, channels))\n",
    "        x = self.drop_path(x)\n",
    "        x = x_skip + x\n",
    "        x_skip = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.drop_path(x)\n",
    "        x = x_skip + x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtract(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size_x = patch_size[0]\n",
    "        self.patch_size_y = patch_size[0]\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n",
    "            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n",
    "            rates=(1, 1, 1, 1),\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dim = patches.shape[-1]\n",
    "        patch_num = patches.shape[1]\n",
    "        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n",
    "\n",
    "\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
    "        super().__init__(name='patch_embed', **kwargs)\n",
    "        self.num_patch = num_patch\n",
    "        self.proj = layers.Dense(embed_dim, name='proj')\n",
    "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim, name='embedding')\n",
    "\n",
    "    def call(self, patch):\n",
    "        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n",
    "        return self.proj(patch) + self.pos_embed(pos)\n",
    "\n",
    "\n",
    "class PatchMerging(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim, prefix=''):\n",
    "        super().__init__()\n",
    "        self.num_patch = num_patch\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False, name=f'{prefix}/downsample/reduction')\n",
    "\n",
    "    def call(self, x):\n",
    "        height, width = self.num_patch\n",
    "        _, _, C = x.get_shape().as_list()\n",
    "        x = tf.reshape(x, shape=(-1, height, width, C))\n",
    "        x0 = x[:, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, :]\n",
    "        x = tf.concat((x0, x1, x2, x3), axis=-1)\n",
    "        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n",
    "        return self.linear_trans(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (3, 3)  # 2-by-2 sized patches\n",
    "dropout_rate = 0.03  # Dropout rate\n",
    "num_heads = 8  # Attention heads\n",
    "embed_dim = 128  # Embedding dimension\n",
    "num_mlp = 256  # MLP layer size\n",
    "qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\n",
    "window_size = 2  # Size of attention window\n",
    "shift_size = 1  # Size of shifting window\n",
    "image_dimension = 150  # Initial image size\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 300\n",
    "validation_split = 0.2\n",
    "weight_decay = 0.0001\n",
    "label_smoothing = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNSwinTransformer3(input_size, patch_size):\n",
    "    inputs = layers.Input(input_size)\n",
    "\n",
    "    \n",
    "    cnn_model = InceptionV3(include_top=False, input_tensor=inputs, weights=None)\n",
    "    cnn_outputs = cnn_model.get_layer('mixed1').output\n",
    "    x1 = layers.GlobalAveragePooling2D(name=\"avg_pool\")(cnn_outputs)\n",
    "    x1 = layers.BatchNormalization()(x1)\n",
    "    x1 = layers.Dense(64)(x1)\n",
    "    \n",
    "    \n",
    "    # swin transformer\n",
    "\n",
    "    input_size2 = input_size\n",
    "        \n",
    "    num_patch_x = input_size2[0] // patch_size[0]\n",
    "    num_patch_y = input_size2[1] // patch_size[1]\n",
    "\n",
    "    x2 = PatchExtract(patch_size)(inputs)\n",
    "    x2 = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x2)\n",
    "\n",
    "    x2 = SwinTransformer_Block(\n",
    "        dim=embed_dim,\n",
    "        num_patch=(num_patch_x, num_patch_y),\n",
    "        num_heads=num_heads,\n",
    "        window_size=window_size,\n",
    "        shift_size=0,\n",
    "        num_mlp=num_mlp,\n",
    "        qkv_bias=qkv_bias,\n",
    "        dropout_rate=dropout_rate,\n",
    "        prefix=\"swinbloc1\"\n",
    "    )(x2)\n",
    "    x2 = SwinTransformer_Block(\n",
    "        dim=embed_dim,\n",
    "        num_patch=(num_patch_x, num_patch_y),\n",
    "        num_heads=num_heads,\n",
    "        window_size=window_size,\n",
    "        shift_size=shift_size,\n",
    "        num_mlp=num_mlp,\n",
    "        qkv_bias=qkv_bias,\n",
    "        dropout_rate=dropout_rate,\n",
    "        prefix=\"swinbloc2\"\n",
    "    )(x2)\n",
    "    \n",
    "\n",
    "    x2 = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim, prefix='patch_merge')(x2)\n",
    "    x2 = layers.GlobalAveragePooling1D()(x2)\n",
    "    x2 = layers.Dense(64)(x2)\n",
    "    \n",
    "    x3 = layers.concatenate([x1,x2])\n",
    "    x3 = layers.Dense(32)(x3)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\",name='output')(x3)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,\n",
    "                              patience=10)\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', patience=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 101\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "cvacc = []\n",
    "cvauc = []\n",
    "\n",
    "i = 1\n",
    "for train, test in kfold.split(data_x2,data_y):\n",
    "    input_size = (150,150,3)\n",
    "    patch_size = (3,3)\n",
    "    model = CNNSwinTransformer3(input_size, patch_size)\n",
    "    model.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
    "    optimizer=tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    ),\n",
    "    metrics=[\n",
    "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "    ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"../Data/models/fold{0}_inceptionv3swintransformer_lung_binary.h5\".format(str(i),'w')\n",
    "    if os.path.exists(checkpoint_filepath):\n",
    "        os.remove(checkpoint_filepath)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath, save_weights_only=True, save_best_only=True, verbose=1,monitor='val_accuracy',mode='max'\n",
    "    )\n",
    "\n",
    "    model.fit(data_x2[train], data_y2[train], validation_split=0.2,epochs=300, batch_size=32, verbose=1,shuffle=True,callbacks=[checkpoint,reduce_lr,earlystop])\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    # evaluate the model\n",
    "    evals = model.evaluate(data_x2[test], data_y2[test], verbose=1)\n",
    "    y_pred = model.predict(data_x2[test])\n",
    "    with open('../Data/models/fold{0}_inceptionv3swintransformer_lung_binary.csv'.format(str(i)),'w') as file:\n",
    "        file.write('true_label,predict_label'+'\\n')\n",
    "        for a,b in zip(data_y2[test],y_pred):\n",
    "            file.write(str(a) + ','+str(b[0])+'\\n')\n",
    "    acc = evals[1]\n",
    " \n",
    "    auc = roc_auc_score(data_y2[test],y_pred)\n",
    "    \n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], acc))\n",
    "    print(\"auc: %.2f\"%(auc))\n",
    "\n",
    "    cvacc.append(acc*100)\n",
    "    cvauc.append(auc)\n",
    "\n",
    "    i+=1\n",
    "print(\"mean acc %.2f%% (+/- %.2f%%)\" % (np.mean(cvacc), np.std(cvacc)))\n",
    "print(\"mean auc %.2f%% (+/- %.2f%%)\" % (np.mean(cvauc), np.std(cvauc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cvacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cvauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,roc_auc_score,recall_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric(gt, pred): \n",
    "    pred2 = []\n",
    "    for i in pred:\n",
    "        if i>0.5:\n",
    "            pred2.append(1)\n",
    "        else:\n",
    "            pred2.append(0)\n",
    "    confusion = confusion_matrix(gt,pred2)\n",
    "    TP = confusion[1, 1]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    acc = (TP+TN)/float(TP+TN+FP+FN)\n",
    "    auc = roc_auc_score(gt,pred)\n",
    "    precision = TP/float(TP+FP)\n",
    "    f1_score_ = f1_score(gt,pred2)\n",
    "    sensitivity = TP / float(TP+FN)\n",
    "    specificity = TN / float(TN+FP)\n",
    "    \n",
    "    return round(acc,4),round(auc,4),round(f1_score_,4), round(precision,4),round(sensitivity,4), round(specificity,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../Data/models/fold1_inceptionv3swintransformer_lung_binary.csv')\n",
    "df1['true_label'] = df1['true_label'].str[1:-1].str.split(' ').str[0].astype(float)\n",
    "\n",
    "calculate_metric(df1['true_label'],df1['predict_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../Data/models/fold2_inceptionv3swintransformer_lung_binary.csv')\n",
    "df2['true_label'] = df2['true_label'].str[1:-1].str.split(' ').str[0].astype(float)\n",
    "\n",
    "calculate_metric(df2['true_label'],df2['predict_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('../Data/models/fold3_inceptionv3swintransformer_lung_binary.csv')\n",
    "df3['true_label'] = df3['true_label'].str[1:-1].str.split(' ').str[0].astype(float)\n",
    "\n",
    "calculate_metric(df3['true_label'],df3['predict_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('../Data/models/fold4_inceptionv3swintransformer_lung_binary.csv')\n",
    "df4['true_label'] = df4['true_label'].str[1:-1].str.split(' ').str[0].astype(float)\n",
    "\n",
    "calculate_metric(df4['true_label'],df4['predict_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('../Data/models/fold5_inceptionv3swintransformer_lung_binary.csv')\n",
    "df5['true_label'] = df5['true_label'].str[1:-1].str.split(' ').str[0].astype(float)\n",
    "\n",
    "calculate_metric(df5['true_label'],df5['predict_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
